{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778fe296-e52c-48e1-89e5-2f5081790d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    PreTrainedTokenizerBase,\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "# Load evaluation metric\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f26be7-410b-49d4-bc7b-b07a9293c0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_split = \"gpt2\"\n",
    "output_name = \"gpt2_RM02\"\n",
    "\n",
    "# Initialize the tokenizer and set the padding token\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_split)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set EOS token as padding token\n",
    "\n",
    "# Define the PEFT configuration for LoRA\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "# Load the model and apply PEFT (LoRA)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name_split, num_labels=1, torch_dtype=torch.bfloat16\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Set padding token in model configuration\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "model.config.use_cache = not False  # Disable caching for compatibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82721776-f44e-464c-9313-cf8092e2d782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "train_df = pd.read_csv('datasets/rewardModelTrain.csv')\n",
    "dataset = Dataset.from_pandas(train_df)\n",
    "\n",
    "# Split the dataset into training and validation\n",
    "train_val_split = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_val_split['train']\n",
    "val_dataset = train_val_split['test']\n",
    "\n",
    "# Define preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    new_examples = {\n",
    "        \"input_ids_j\": [],\n",
    "        \"attention_mask_j\": [],\n",
    "        \"input_ids_k\": [],\n",
    "        \"attention_mask_k\": [],\n",
    "    }\n",
    "    for question, response_j, response_k in zip(examples[\"question\"], examples[\"response_j\"], examples[\"output_1\"]):\n",
    "        tokenized_j = tokenizer(\"Question: \" + question + \"\\n\\nAnswer: \" + str(response_j), truncation=True)\n",
    "        tokenized_k = tokenizer(\"Question: \" + question + \"\\n\\nAnswer: \" + str(response_k), truncation=True)\n",
    "\n",
    "        new_examples[\"input_ids_j\"].append(tokenized_j[\"input_ids\"])\n",
    "        new_examples[\"attention_mask_j\"].append(tokenized_j[\"attention_mask\"])\n",
    "        new_examples[\"input_ids_k\"].append(tokenized_k[\"input_ids\"])\n",
    "        new_examples[\"attention_mask_k\"].append(tokenized_k[\"attention_mask\"])\n",
    "\n",
    "    return new_examples\n",
    "\n",
    "# Apply preprocessing\n",
    "num_proc = 24  # Adjust based on your machine's capability\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=num_proc,\n",
    "    remove_columns=train_dataset.column_names,\n",
    ")\n",
    "train_dataset = train_dataset.filter(\n",
    "    lambda x: len(x[\"input_ids_j\"]) <= 512 and len(x[\"input_ids_k\"]) <= 512\n",
    ")\n",
    "eval_dataset = val_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=num_proc,\n",
    "    remove_columns=val_dataset.column_names,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9274e8-1b7d-4afd-86cc-a41f5c529e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RewardDataCollatorWithPadding:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = 512\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        features_j = []\n",
    "        features_k = []\n",
    "        for feature in features:\n",
    "            features_j.append(\n",
    "                {\n",
    "                    \"input_ids\": feature[\"input_ids_j\"],\n",
    "                    \"attention_mask\": feature[\"attention_mask_j\"],\n",
    "                }\n",
    "            )\n",
    "            features_k.append(\n",
    "                {\n",
    "                    \"input_ids\": feature[\"input_ids_k\"],\n",
    "                    \"attention_mask\": feature[\"attention_mask_k\"],\n",
    "                }\n",
    "            )\n",
    "        batch_j = self.tokenizer.pad(\n",
    "            features_j,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=self.return_tensors,\n",
    "        )\n",
    "        batch_k = self.tokenizer.pad(\n",
    "            features_k,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=self.return_tensors,\n",
    "        )\n",
    "        batch = {\n",
    "            \"input_ids_j\": batch_j[\"input_ids\"],\n",
    "            \"attention_mask_j\": batch_j[\"attention_mask\"],\n",
    "            \"input_ids_k\": batch_k[\"input_ids\"],\n",
    "            \"attention_mask_k\": batch_k[\"attention_mask\"],\n",
    "            \"return_loss\": True,\n",
    "        }\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996afe0d-f2c2-4e66-b65d-a85ff30053c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        rewards_j = model(input_ids=inputs[\"input_ids_j\"], attention_mask=inputs[\"attention_mask_j\"])[0]\n",
    "        rewards_k = model(input_ids=inputs[\"input_ids_k\"], attention_mask=inputs[\"attention_mask_k\"])[0]\n",
    "        loss = -nn.functional.logsigmoid(rewards_j - rewards_k).mean()\n",
    "        if return_outputs:\n",
    "            return loss, {\"rewards_j\": rewards_j, \"rewards_k\": rewards_k}\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa916eb3-ecbe-4bc1-9d0e-da7091bde77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, _ = eval_pred\n",
    "    # Here, predictions is rewards_j and rewards_k.\n",
    "    # We want to see how much of the time rewards_j > rewards_k.\n",
    "    predictions = np.argmax(predictions, axis=0)\n",
    "    labels = np.zeros(predictions.shape)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab84fbd3-a074-442f-acc5-c1f751607e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_name,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=3,  # Increase the number of epochs if needed\n",
    "    weight_decay=0.001,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=False,\n",
    "    deepspeed=None,\n",
    "    local_rank=-1,\n",
    "    remove_unused_columns=False,  # Important for custom inputs\n",
    "    bf16=True,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    optim=\"adamw_hf\",\n",
    "    lr_scheduler_type=\"linear\",\n",
    ")\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=RewardDataCollatorWithPadding(tokenizer=tokenizer, max_length=512),\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train(False)\n",
    "\n",
    "# Save the trained model\n",
    "model.save_pretrained(output_name + \"_peft_last_checkpoint\")\n",
    "trainer.model.save_pretrained(\"reward_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9977b4-9466-44b6-9d8d-1a9afdc5636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from accelerate import Accelerator\n",
    "\n",
    "current_device = Accelerator().local_process_index\n",
    "sentiment_pipe = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=model,\n",
    "    device_map={\"\": current_device},\n",
    "    model_kwargs={\"load_in_8bit\": True},\n",
    "    tokenizer=tokenizer,\n",
    "    return_token_type_ids=False,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "text = train_df['question'].iloc[161] + train_df['output_1'].iloc[161]  \n",
    "inputs = tokenizer(str(text), return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "\n",
    "pipe_outputs = sentiment_pipe(text)\n",
    "reward_baseline = 0.5  # Example value, adjust as needed\n",
    "\n",
    "rewards = [torch.tensor(output[\"score\"] - reward_baseline) for output in pipe_outputs]\n",
    "rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a670d822-1b00-4b63-946f-2b7e666ba076",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparqlgen",
   "language": "python",
   "name": "sparqlgen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
